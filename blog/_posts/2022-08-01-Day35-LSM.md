---
layout: post
title: Day 35 - Echo State Networks/ Liquid State Machines
tags: [ICL Project]
include_toc: true
---

# Liquid State Machines vs ESN
> "Three different uses of a recurrent neural network (RNN) as a reservoir that is not trained but instead read out 
> by a simple external classification layer have been described in the literature: Liquid State Machines (LSMs), 
> Echo State Networks (ESNs) and the Backpropagation Decorrelation (BPDC) learning rule" . Check this review paper on 
> [reservoir 
> computing methods](https://www.sciencedirect.com/science/article/pii/S089360800700038X)


- LSM and ESM are two algorithms that were proposed by two research groups independently. These algorithms are 
  computationally 
  light 
 and belong to the category of reservoir computing (LC). ESN is proposed by Jaeger 2001 and LSM by 
  Masass in 2002. The main difference is that LSM is biologically inspired SNN  whereas EN is a rate-based 
  approximation.
  - LSM consists of 3 layers: input, liquid layer and readout.
  - Liquid layer is recurrently connected which allows for storing information in time. In other words, these 
    network maintain some fading memory of the past. The main 
    advantage of those 
    networks is that "all synaptic connections except for those which connect to the readout layer, are randomly 
    initialized and remain fixed" [check this paper](https://www.frontiersin.org/articles/10.3389/fnins.2019.00686/full)


- Why do we talk about efficiency of SNN and why are they more suitable for edge computing?
  - Mainly because you can store the neuronal  activation's in a single bit (all-or none) which can lead to gains 
    in power 
    efficiency. Neftci et al. in 2017 reported a power consumption as low as 20 pJ per spike 


- There is a lot of research on how to build ESN that can capture information in longer time-scales. The shallow 
  vanilla reservoir networks can only capture information in short time-scales. To overcome this, multiple versions 
  of hierarchical ESN were introduced by many research groups. The idea is to have some earlier layers processing 
  information on a fast timescale, while final layers would process information on a slower timesclae. Some examples:
  - To develop hierarchical information processing system, many reservoir networks are connected sequentially. The 
    output of each reservoir feeds in to the next. Predictions are taken as a combination of the output of the 
    different reservoirs.
  - Another topology proposed in 2010, many trained ESN are stacked on top of each other creating a hierarchical 
    chain of reservoirs. Intermediary layers are trained to perform the task (i.e the readout layer of the 
    intermediary reservoir is trained)

## Properties of reservoir networks
- What we mean by recurrency is that at time t, a neuron is receiving/is stimulated by input vector (layer) at 
  time t 
  and by 
  other neurons in the reservoir (which holds state t-1)
- What a reservoir of neuron is doing eventually is creating a high-dimensional representation of the input.