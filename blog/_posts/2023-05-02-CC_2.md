---
layout: post
title: Day 2 - Fast and Slow Learning
tags: [CapoCaccia Workshop 23 - Sessions Notes]
include_toc: true
---

## Morning Session 

### Where knowledge come from?


- How to know what problems fish solve :D?
    - look into evolution: what conditions compromised their existence over time. What's necessary for them to survive?
    - Observe them in their natural environment.

- What Skills animals need to solve problems. How do they acquire these skills: Learning is changing in behavior based on contents
    1. Innate or genetic: born with it: this is zero-shot learning? [long-term , minutes/hours]
        - Florian: we are born with all teh knowledge we need. Genetic knowledge-based
        - No change in synaptic weights? but needs to be revealed
    2. Single-shot learning  
        - Doens't happen that often: its rare for mice/mammmals. ex: aversive food-conditionning. (few bits)
        - Learning means: changing behavior
        - episodic and semantic memeory fall
        - Occurs when there is  a big error signal
    3. Ongoing calibration [short-term, seconds]
        - Difference to 2. this one serves for stability (homeostatic feedback loop)
        - Measures error signals, animals try to find a local minimum in the action space. Re-calbrates to maintain stability.
        - This happens constantly and without it drift happens.
        - When the animal tries to stabilize and not update new information into the system
        - Incremental and required repetition
        - Snakk error 

- So what is mechanistically different between the 3 types of learning?


- Habit formation: offload computation. Habit is stererotypical
    - How to create that on chip?
    - How to change bad habits/ switch from a habit:  one option is with strong one-shot learning: have sth that could shake the system
        - Give a bigger reward to a different behavior


## Morning session #2
**Richard Naud**

- In Hippocampus, we need a burst on the post-synaptic cell to create potentiation (LTP)
    - We get very little plasticity with single spikes (one spike on pre and one on post): depression
    - In the contrast, when we pair pre with post (burt of spikes) --> LTP

**Emre Neftci**

- Learning rules: locality and continum
- Bi-level learning: 
    1. Train system offline: simulate the innate/generitc + fine tuning
    2. Do a single shot

- How to train?
"Fast and Slow Learning"
- Offline: $argmin_w Loss(f(x,w'))$ : done by gradient descent
- Online: incremental: $w' = w + \delta w(w)$

    - Example of $\delta w$ is STDP
    - What happens is not waht we wanted which is learning to learn but rather these networks learn a good representation that would allow is to do the fast learning part
    - why are we min f(x,w') w' as well? because we can also add the hardware constrainst such as weight quantization
    - We don't have to fix the form of delta w

- Unlocking behavior 
    - The issue with 1-shot learning: is that as we do weight update we can forget/ overwritting --> we are messing up with the network
    - Zero-shot learning: ZSL
    - Ex: Contrastive image language




## Afternoon
**Micheal: Single-shot learning**
- Single-pass vs. single-shot
    - pass: "once through the dataset"
    - shot: one example from a new class.
    - Continuous learning: online learning
        - adapting to new data
        - somewhat unsupervised



# Evening: Why so many neuromorphic chips?
Check Carver Mead's new paper: check quote from Misha's paper

- Silicon Cortex: Rodney: 1993, a series of chips which carries multiple HH neurons, layed on local AER bus, both asynchr and syncrhon (herarchical bus). Floating gate. Mixed system. The motivation of this was to follow up on the silicon neuron. They wanted to transport waht they learned about anatomy on chip
- TrueNorth: IBM chip first single chip with 1M spiking neurons, fully asynchronous, 2014? tiem multiplexed asynchronous logic, 4000 cores working in parallel (time-multiplexed), each core has 256 neurons. Funded by DARPA
- Spinnaker: 2010, fully digitial., locally sync(whithin core )globally async (communications across). Design philo: because its fully digital, it was fr research, flexible on software ,a scalable research platform, just to be able to explore different neuron models just from the software. 256 M 
- ROLLS: 2008, 256 neurons, 256X256 sybapses non plastic + 256 memry array (plastic synapses) (has learning inspired by stefano fusi). A bottom up approach, mixed signal, async interface, 
- BrainScales: 2005, 50M synapses per wafer, mixed singal, accelerated. Idea:above threshold neuron that can work in accelerated time. Stay as close as possible to the time sclae of the differential equation, to be able to ...
- DYNAPSE: 4 cores, 256, bottom up approach in designing, explore what dynamics can come up from collections of synapses, can receive input from real-time. 2012
- TIANJIC: ANN accelerated chip, on chip there is a SNN simulator, 
- ODIN:
- LOIHI:
- GML
- Brainchip
